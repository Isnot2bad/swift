//
//  ScreenRecorder.swift
//  VideoDecode
//
//  Created by larryhou on 10/01/2018.
//  Copyright Â© 2018 larryhou. All rights reserved.
//

import Foundation
import ReplayKit
import AVKit
import UIKit.UIGestureRecognizerSubclass

@available(iOS 10.0, *)
extension RPSampleBufferType: CustomStringConvertible {
    public var description: String {
        switch self {
            case .audioApp:return "audio_app"
            case .audioMic:return "audio_mic"
            case .video:return "video"
        }
    }
}

@objc public class MovieEditInstruction: NSObject {
    let range: CMTimeRange
    let comment: String

    init(range: CMTimeRange, comment: String) {
        self.range = range
        self.comment = comment
    }
}

class UITouchGestureRecognizer: UIGestureRecognizer {
    override func touchesBegan(_ touches: Set<UITouch>, with event: UIEvent) {
        self.state = .began
    }

    override func touchesMoved(_ touches: Set<UITouch>, with event: UIEvent) {
        self.state = .changed
    }

    override func touchesEnded(_ touches: Set<UITouch>, with event: UIEvent) {
        self.state = .ended
    }

    override func touchesCancelled(_ touches: Set<UITouch>, with event: UIEvent) {
        self.state = .cancelled
    }
}

extension Double {
    var seconds: Double {return floor(self * Double(PREFER_TIMESCALE)) / Double(PREFER_TIMESCALE)}
}

@objc public enum ScreenRecordStatus: Int {
    case idle, starting, recording, stopping, editing, complete, error
    public var description: String {
        switch self {
            case .idle: return "idle"
            case .starting: return "starting"
            case .recording: return "recording"
            case .stopping: return "stopping"
            case .editing: return "editing"
            case .complete: return "complete"
            case .error: return "error"
        }
    }
}

private let recordMovie = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)[0].appendingPathComponent("record.mp4")

@available(iOS 11.0, *)
public class ScreenRecorder: NSObject {
    @objc public static let shared: ScreenRecorder = ScreenRecorder(cameraViewport: CGSize(width: 100, height: 100))

    private let cameraViewport: CGSize
    private var writer: AssetWriter!

    private var syncdata: [[Double]] = []
    private let composedMovie: URL

    @objc public var statusObserver: ((ScreenRecordStatus) -> Void)?
    @objc public var status: ScreenRecordStatus = .idle {
        didSet { statusObserver?(status) }
    }

    init(cameraViewport: CGSize = CGSize(width: 50, height: 50)) {
        self.cameraViewport = cameraViewport
        self.composedMovie = AssetManager.shared.composedMovie
    }

    @objc public var isRecording: Bool { return RPScreenRecorder.shared().isRecording }
    @objc public var reviewEnabled = false

    @objc public func cleanup() {
        AssetManager.shared.cleanupStorage(type: .editor)
        do {try FileManager.default.removeItem(at: recordMovie)} catch {debug.print(error)}
    }

    @objc public func requestCameraAuthorization(completion:@escaping () -> Void) {
        requestCaptureAuthorization(for: .video, completion: completion)
    }

    @objc public func requestMicrophoneAuthorization(completion:@escaping () -> Void) {
        requestCaptureAuthorization(for: .audio, completion: completion)
    }

    private func requestCaptureAuthorization(for type: AVMediaType, completion:@escaping () -> Void) {
        let status = AVCaptureDevice.authorizationStatus(for: type)
        if status == .authorized {
            completion()
        } else {
            AVCaptureDevice.requestAccess(for: type) { (granted) in
                if granted {completion()}
            }
        }
    }

    @objc public func startRecording(completion: ((Error?) -> Void)? = nil) {
        writer = try! AssetWriter(url: recordMovie)

        synctime = kCMTimeZero
        syncdata.removeAll()

        debug.print("[ScreenRecorder]", "startRecording(completion:)", "availableSpace:\(AssetManager.shared.availableSpace) recordingAvailable:\(AssetManager.shared.recordingAvailable)")
        status = .starting
        let recorder = RPScreenRecorder.shared()
        recorder.startCapture(handler: receiveScreenSample(sample:type:error:)) { (error) in
            completion?(error)
            debug.print("[ScreenRecorder]", "RPScreenRecorder.startCapture(handler:completionHandler:)", error ?? "success")
            self.status = error == nil ? .recording : .error
            guard error == nil else {return}
            DispatchQueue.main.async {
                self.setupCamera()
                if let cameraView = recorder.cameraPreviewView {
                    cameraView.frame = CGRect(origin: CGPoint.zero, size: self.cameraViewport)
                    cameraView.mask = self.drawCameraShape(size: self.cameraViewport)
                    if let rootView = UIApplication.shared.keyWindow?.rootViewController?.view {
                        let options: UIViewAnimationOptions = [.curveLinear, .transitionCrossDissolve]
                        UIView.transition(with: rootView, duration: 1.0, options: options, animations: {
                            rootView.addSubview(cameraView)
                        }, completion: nil)
                    }
                }
            }
        }
    }

    // MARK: camera
    private func setupCamera() {
        guard let cameraView = RPScreenRecorder.shared().cameraPreviewView else {return}
        if cameraView.constraints.count > 0 {return}

        debug.print("[ScreenRecorder]", "setupCamera")

        let tap = UITouchGestureRecognizer(target: self, action: #selector(changeCamera(_:)))
        cameraView.addGestureRecognizer(tap)

        let pan = UIPanGestureRecognizer(target: self, action: #selector(moveCameraPreview(_:)))
        pan.maximumNumberOfTouches = 1
        cameraView.addGestureRecognizer(pan)
        tap.require(toFail: pan)
    }

    @objc private func moveCameraPreview(_ gesture: UIPanGestureRecognizer) {
        guard let view = gesture.view else {return}
        switch gesture.state {
            case .began:
                gesture.setTranslation(CGPoint.zero, in: view)
            case .changed:
                let translation = gesture.translation(in: view)
                view.frame = view.frame.offsetBy(dx: translation.x, dy: translation.y)
                gesture.setTranslation(CGPoint.zero, in: view)
            case .ended:
                let bounds = UIScreen.main.bounds, frame = view.frame
                var offset = CGPoint.zero
                if frame.maxX > bounds.width {
                    offset.x = bounds.width - frame.maxX
                } else if frame.minX < 0 {
                    offset.x = -frame.minX
                }

                if frame.maxY > bounds.height {
                    offset.y = bounds.height - frame.maxY
                } else if frame.minY < 0 {
                    offset.y = -frame.minY
                }
                UIView.animate(withDuration: 0.5) {
                    view.frame = frame.offsetBy(dx: offset.x, dy: offset.y)
                }
            default:break
        }
    }

    @objc private func changeCamera(_ gesture: UITouchGestureRecognizer) {
        guard gesture.state == .began else { return }
        if gesture.numberOfTouches >= 2 {
            let position = RPScreenRecorder.shared().cameraPosition
            switch position {
                case .back: RPScreenRecorder.shared().cameraPosition = .front
                case .front: RPScreenRecorder.shared().cameraPosition = .back
            }
        } else {
            if let view = RPScreenRecorder.shared().cameraPreviewView {
                let alpha: CGFloat
                if view.alpha == 1.0 {
                    alpha = 0.2
                } else {
                    alpha = 1.0
                }

                view.layer.removeAllAnimations()
                UIView.animate(withDuration: 0.2) {
                    view.alpha = alpha
                }
            }
        }
    }

    private func drawCameraShape(size: CGSize) -> UIImageView? {
        UIGraphicsBeginImageContext(size)
        guard let context = UIGraphicsGetCurrentContext() else { return nil }
        UIColor.black.setFill()
        context.addEllipse(in: CGRect(origin: CGPoint.zero, size: size))
        context.fillPath()
        let image = UIGraphicsGetImageFromCurrentImageContext()
        UIGraphicsEndImageContext()
        return UIImageView(image: image)
    }

    // MARK: sample
    private var synctime: CMTime = kCMTimeZero
    private func receiveScreenSample(sample: CMSampleBuffer, type: RPSampleBufferType, error: Error?) {
        guard status == .recording else {return}
        if !synctime.isValid {synctime = kCMTimeZero}
        if error == nil {
            writer.append(sample: sample, type: type)
            if type == .video {
                let position = CMSampleBufferGetPresentationTimeStamp(sample) - writer.timeOffset
                if Int(position.seconds) % 5 == 0 && (position - synctime).seconds >= 1 {
                    synctime = position
                    synchronize(timestamp: position.seconds)
                }
            }
        } else {
            status = .error
            debug.print(error ?? sample)
        }
    }

    @objc public var requestUnityTime:(() -> Void)? // get game timestamp for timesync
    private func synchronize(timestamp value: Double) {
        print("[ScreenRecorder]", "synchronize(timestamp:)", value)
        requestUnityTime?()
    }

    @objc public func appendUnityTime(_ value: Double) {
        print("[ScreenRecorder]", "appendUnityTime(_:)", value, synctime.seconds - value)
        if !value.isNaN && !synctime.seconds.isNaN {
            syncdata.append([synctime.seconds, value])
        }
    }

    @objc public func startMovieEditProcess(clipContext: String? = nil, summary: String? = nil, keepClips: Bool, completion: ((URL, String?) -> Void)? = nil) {
        guard let clipContext = clipContext, clipContext.count > 0 else {
            status = .error
            debug.print("[ScreenRecorder]", "startExportSession(clipContext:completion:) invalid context")
            completion?(recordMovie, nil)
            return
        }

        status = .editing
        AssetManager.shared.cleanupStorage(type: .editor)

        debug.print("[ScreenRecorder]", "startExportSession(clipContext:completion:) call", clipContext)

        let offset: Double
        if syncdata.count > 0 {
            let samples = syncdata.map({ $0[0] - $0[1] })
            offset = samples.reduce(0, { $0 + $1 }) / Double(samples.count)
            debug.print("[ScreenRecorder]", "SYNC", samples, "OFFSET", offset)
        } else {
            offset = 0
        }

        let instructions = clipContext.split(separator: ";").map { (item) -> MovieEditInstruction in
            let data = item.split(separator: "|")
            let pair = data[1].split(separator: "-").map({Double($0) ?? .nan})
            let f = CMTime(seconds: (pair[0] + offset).seconds, preferredTimescale: PREFER_TIMESCALE)
            let t = CMTime(seconds: (pair[1] + offset).seconds, preferredTimescale: PREFER_TIMESCALE)
            return MovieEditInstruction(range: CMTimeRange(start: f, end: t), comment: String(data[0]))
        }.filter({$0.range.start.value >= 0})

        if FileManager.default.fileExists(atPath: composedMovie.path) {
            try? FileManager.default.removeItem(at: composedMovie)
        }

        let editor = MovieEditor(transition: .dissolve, transitionDuration: 1.0)
        let attributes = AssetManager.shared.attributes(of: recordMovie.path)
        debug.print("[ScreenRecorder]", "MovieEditor.cut(asset:with:summary:exportClips:transition:completion:) call attributes:\(attributes)")
        editor.cut(asset: AVAsset(url: recordMovie), with: instructions, summary: summary, exportClips: keepClips) {
            let editJSON = AssetManager.shared.editJSON
            self.status = editJSON == nil ? .error : .complete
            debug.print("[ScreenRecorder]", "MovieEditor.cut(asset:with:summary:exportClips:transition:completion:) completion", editJSON ?? "nil")
            completion?(self.composedMovie, editJSON)
            if self.reviewEnabled {
                self.reviewMovie(self.composedMovie)
            }

//            do{try FileManager.default.removeItem(at: recordMovie)}catch{logger.print(error)}
        }
    }

    private var exporter: AVAssetExportSession!
    @objc public func stopRecording(clipContext: String? = nil, summary: String? = nil, completion: ((URL, String?) -> Void)? = nil, keepClips: Bool = true) {
        if !isRecording {
            status = .error
            completion?(recordMovie, nil)
            return
        }

        synchronize(timestamp: .nan)

        status = .stopping
        debug.print("[ScreenRecorder]", "stopRecording(clipContext:completion:) call", clipContext ?? "clipContext = nil")
        RPScreenRecorder.shared().stopCapture { (error) in
            debug.print("[ScreenRecorder]", "RPScreenRecorder.stopCapture(handler:)", error ?? "success")
            DispatchQueue.main.async {
                self.writer.save {
                    self.startMovieEditProcess(clipContext: clipContext, summary: summary, keepClips: keepClips, completion: completion)
                }

                if let cameraView = RPScreenRecorder.shared().cameraPreviewView, let rootView = cameraView.superview {
                    let options: UIViewAnimationOptions = [.curveLinear, .transitionCrossDissolve]
                    UIView.transition(with: rootView, duration: 0.25, options: options, animations: {
                        cameraView.removeFromSuperview()
                    }, completion: nil)
                }
            }
        }
    }

    private func reviewMovie(_ url: URL) {
        debug.print("[ScreenRecorder]", "reviewMovie(_:)")
        AssetManager.shared.playMovie(with: url)
    }

    @objc public var progressObserver: ((Float) -> Void)?
    @objc private func progressUpdate(_ timer: Timer) {
//        progressObserver?(exporter.progress)
//        if exporter.progress == 1.0
//        {
//            timer.invalidate()
//        }
    }

    @objc public func encode(command: String, data: String) -> String? {
        let encoder = JSONEncoder()
        if let data = try? encoder.encode(MessageProtocol(command: command, data: data)) {
            return String(data: data, encoding: .utf8)
        }
        return nil
    }
}

extension AVAssetWriterStatus: CustomStringConvertible {
    public var description: String {
        switch self {
            case .cancelled:return "cancelled"
            case .completed:return "completed"
            case .writing:return "writing"
            case .unknown:return "unknown"
            case .failed:return "failed"
        }
    }
}

@available(iOS 11.0, *)
class AssetWriter {
    private let writer: AVAssetWriter
    let url: URL

    private var movieTracks: [RPSampleBufferType: AVAssetWriterInput] = [:]

    init(url: URL) throws {
        self.url = url
        if FileManager.default.fileExists(atPath: url.path) {
            try? FileManager.default.removeItem(at: url)
        }

        self.writer = try AVAssetWriter(url: url, fileType: .mp4)
        self.writer.movieTimeScale = 600
    }

    private var initialized = false
    private func setup(sample: CMSampleBuffer) {
        if (initialized) { return }
        initialized = true

        let screen = UIScreen.main.bounds
        let width: CGFloat = 720
        let height = round(width * screen.height / screen.width)

        let videoOptions: [String: Any] = [
            AVVideoCodecKey: AVVideoCodecType.h264,
            AVVideoWidthKey: width, AVVideoHeightKey: height]
        movieTracks[.video] = AVAssetWriterInput(mediaType: .video, outputSettings: videoOptions)

        if let format = CMSampleBufferGetFormatDescription(sample), let stream = CMAudioFormatDescriptionGetStreamBasicDescription(format) {
            let audioOptions: [String: Any] = [
                AVFormatIDKey: kAudioFormatMPEG4AAC,
                AVNumberOfChannelsKey: stream.pointee.mChannelsPerFrame,
                AVSampleRateKey: stream.pointee.mSampleRate]
            movieTracks[.audioApp] = AVAssetWriterInput(mediaType: .audio, outputSettings: audioOptions)
            if RPScreenRecorder.shared().isMicrophoneEnabled {
                movieTracks[.audioMic] = AVAssetWriterInput(mediaType: .audio, outputSettings: audioOptions)
            }
        } else {
            let audioOptions: [String: Any] = [
                AVFormatIDKey: kAudioFormatMPEG4AAC,
                AVNumberOfChannelsKey: 1,
                AVSampleRateKey: 44100]
            movieTracks[.audioApp] = AVAssetWriterInput(mediaType: .audio, outputSettings: audioOptions)
        }

        for (type, track) in movieTracks {
            track.expectsMediaDataInRealTime = true
            if writer.canAdd(track) {
                writer.add(track)
            } else {
                debug.print("[AssetWriter] AVAssetWriter.add(_:AVAssetWriterInput#\(type)) failure")
            }
        }

        timeOffset = CMSampleBufferGetPresentationTimeStamp(sample)

        writer.startWriting()
        writer.startSession(atSourceTime: timeOffset)
        debug.print("[AssetWriter]", "AVAssetWriter.startSession(atSourceTime:\(timeOffset.seconds)")
        assetWriting = true
    }

    private(set) var timeOffset: CMTime = kCMTimeZero

    private let background = DispatchQueue(label: "video_encode_queue")
    func append(sample: CMSampleBuffer, type: RPSampleBufferType) {
        guard CMSampleBufferIsValid(sample) else {return}

        background.sync {
            if !initialized {
                setup(sample: sample)
            }

            guard writer.status == .writing, assetWriting else {return}

            if let track = movieTracks[type], track.isReadyForMoreMediaData {
                if type == .video {
                    track.append(sample)
                    if let error = writer.error {
                        debug.print("[AssetWriter] append(sample:type:\(type)", error)
                    }
                } else {
                    track.append(sample)
                    if let error = writer.error {
                        debug.print("[AssetWriter] append(sample:type:\(type)", error)
                    }
                }
            }
        }
    }

    private var assetWriting: Bool = false
    func save(completion:(() -> Void)? = nil) {
        assetWriting = false
        debug.print("[AssetWriter] save(completion:)", "call status:\(self.writer.status)")
        guard self.writer.status == .writing else {
            debug.print("[AssetWriter] save(completion:)", "failure status:\(self.writer.status)")
            return
        }

        self.writer.finishWriting {
            debug.print("[AssetWriter] save(completion:)", "finish status:\(self.writer.status)", self.writer.error ?? "success")
            completion?()
        }
    }
}

struct MessageProtocol: Codable {
    let command: String
    let data: String?
}
